import SciLean

import SciLean.Core.Distribution.Basic
import SciLean.Core.Distribution.ParametricDistribDeriv

namespace SciLean

open Rand MeasureTheory

variable {R} [RealScalar R] [MeasureSpace R]
set_default_scalar R



----------------------------------------------------------------------------------------------------
-- Variational Inference - Test 1 ------------------------------------------------------------------
----------------------------------------------------------------------------------------------------

def model1 :=
  let v ~ normal (0:R) 5
  if v > 0 then
    let obs ~ normal (1:R) 1
  else
    let obs ~ normal (-2:R) 1


def guide1 (Œ∏ : R) := normal Œ∏ 1


noncomputable
def loss1 (Œ∏ : R) := KLDiv (R:=R) (guide1 Œ∏) (model1.conditionSnd 0)


variable
  {W} [Vec R W]
  {X} [MeasurableSpace X] [Vec R X]
  {Y} [Vec R Y] [Module ‚Ñù Y]

@[fun_trans]
theorem Rand.ùîº.arg_r.cderiv_rule (r : W ‚Üí Rand X) (f : X ‚Üí Y) :
  cderiv R (fun w => (r w).ùîº f)
  =
  fun w dw =>
    let d := parDistribDeriv (fun w => (r w).‚Ñô.toDistribution (R:=R)) w dw
    d.extAction f (fun r ‚ä∏ fun y ‚ä∏ ((r ‚Ä¢ y) : Y)) := sorry_proof

set_option trace.Meta.Tactic.fun_trans true in

/-- Compute derivative of `loss1` by directly differentiating KLDivergence -/
def loss1_grad := (‚àÇ Œ∏ : R, loss1 Œ∏) rewrite_by
  unfold loss1
  unfold scalarCDeriv
  -- unfold scalarCDeriv
  -- rw[KLDiv.arg_P.cderiv_rule]

  -- fun_trans
  simp only [kldiv_elbo]
  fun_trans [Tactic.if_pull]
  unfold model1 guide1 ELBO
  fun_trans [Tactic.if_pull]
